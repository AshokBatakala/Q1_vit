{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVZW3xQVsmqt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as f\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "\n",
        "from torch import nn\n",
        "from torch import tensor\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision.transforms import Compose, Resize, ToTensor,transforms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjRycu3xsmqv"
      },
      "source": [
        "### Multi-Layer Perceptron class  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6YDwSpQsmqw"
      },
      "outputs": [],
      "source": [
        "class MultilayerPerceptron(nn.Module):\n",
        "    \"\"\"Multi Layer perceptron with one hidden layer\"\"\"\n",
        "    def __init__(self, in_dim:  int, hidden_dim:int,out_dim:int,droupout_p : float) -> None:\n",
        "        super(MultilayerPerceptron,self).__init__()\n",
        "        \n",
        "        \n",
        "        self.lin_1 = nn.Linear(in_features=in_dim,out_features=hidden_dim)\n",
        "        self.act_1 = nn.GELU()\n",
        "        self.droupout_1=nn.Dropout(p=droupout_p)\n",
        "        self.lin_2 = nn.Linear(in_features=hidden_dim,out_features=out_dim)\n",
        "        self.droupout_2 = nn.Dropout(p=droupout_p)\n",
        "        \n",
        "        \n",
        "    def forward(self,input: tensor) -> tensor:\n",
        "        \n",
        "        \n",
        "        output = self.lin_1(input)\n",
        "        output = self.act_1(output)\n",
        "        output = self.droupout_1(output)\n",
        "        output = self.lin_2(output)\n",
        "        output = self.droupout_2(output)\n",
        "        \n",
        "        return output    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqn_3K-wsmqy"
      },
      "source": [
        "### Tokenizer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5biuaeKfsmqz"
      },
      "outputs": [],
      "source": [
        "class Tokenizer(nn.Module):\n",
        "    \"\"\"Tokenizes the image\"\"\"\n",
        "    \n",
        "    def __init__(self, token_dim: int, patch_size: int) -> None:\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_to_tokens = nn.Conv2d(in_channels=3,out_channels=token_dim,kernel_size=patch_size,stride=patch_size)\n",
        "        \n",
        "        \n",
        "    def forward(self, input:tensor)->tensor:\n",
        "        \n",
        "        \"\"\"Returns token in shape of (batch_size, n_token, token_dim)\"\"\"\n",
        "        \n",
        "        output = self.input_to_tokens(input)\n",
        "        output = torch.flatten(output,start_dim=-2,end_dim=-1)\n",
        "        output = output.transpose(-2,-1)\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    \n",
        "class ClasstokenConcatenator(nn.Module):\n",
        "    \"\"\"Concatenate the Class with set of tokens\"\"\"\n",
        "    \n",
        "    def __init__(self, token_dim: int) -> None:\n",
        "        \n",
        "        super().__init__()\n",
        "        self.class_token = nn.Parameter(torch.zeros(token_dim))\n",
        "        \n",
        "    def forward(self, input:tensor) -> tensor:\n",
        "\n",
        "        class_token = self.class_token.expand(len(input),1,-1)\n",
        "        output = torch.cat((class_token,input),dim=1)\n",
        "                    \n",
        "        return output\n",
        "    \n",
        "    \n",
        "class PositionEmbeddingAdder(nn.Module):\n",
        "    \"\"\"adds learnable parameters to token for position embedding\"\"\"\n",
        "    \n",
        "    def __init__(self, n_token: int, token_dim: int) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        position_embedding = torch.zeros(n_token,token_dim)\n",
        "        self.position_embedding =  nn.Parameter(position_embedding)\n",
        "        \n",
        "    def forward(self, input:tensor)->tensor:\n",
        "        \n",
        "        output = input+self.position_embedding\n",
        "        return output\n",
        "                    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvimNxPxsmq0"
      },
      "source": [
        "### Attention Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-CJ2jPosmq1"
      },
      "outputs": [],
      "source": [
        "class QueriesKeyValuesExtractor(nn.Module):\n",
        "    \"\"\"get queries key value from multi head self attention\"\"\"\n",
        "    \n",
        "    def __init__(self,token_dim:int,head_dim:int,n_heads:int) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        self.head_dim = head_dim\n",
        "        self.n_heads = n_heads\n",
        "        queries_key_values_dim = 3*self.n_heads*self.head_dim\n",
        "        \n",
        "        self.input_to_queries_key_values = nn.Linear(in_features=token_dim,out_features=queries_key_values_dim,bias = False)\n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self,input: tensor) -> tuple[tensor,tensor,tensor]:\n",
        "        \n",
        "        \n",
        "        batch_size,n_token,token_dim = input.shape\n",
        "        queries_key_values = self.input_to_queries_key_values(input)            #input -> [batch_size, n_tokens, token_dim]\n",
        "        queries_key_values = queries_key_values.reshape(batch_size,3,self.n_heads,n_token,self.head_dim)\n",
        "        \n",
        "        queries, keys, values = queries_key_values.unbind(dim=1)\n",
        "        \n",
        "        return queries, keys, values\n",
        "    \n",
        "    \n",
        "def get_attention(queries: tensor, keys: tensor, values: tensor) -> tensor:\n",
        "        \n",
        "        \n",
        "    scale = queries.shape[-1]**(-0.5)\n",
        "    attention_scores = (queries @  keys.transpose(-1,-2)) * scale\n",
        "        \n",
        "    attention_prob = f.softmax(attention_scores,dim=-1)\n",
        "        \n",
        "    attention = attention_prob @ values\n",
        "        \n",
        "    return attention\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_zM9c7ysmq2"
      },
      "outputs": [],
      "source": [
        "class Multiheadselfattention(nn.Module):\n",
        "    \"\"\"Multi head self attention\"\"\"\n",
        "    \n",
        "    def __init__(self,token_dim: int , head_dim: int , n_heads : int, droupout_p : float) -> None:\n",
        "        super(Multiheadselfattention,self).__init__()\n",
        "        \n",
        "        \n",
        "        self.query_key_value_extractor = QueriesKeyValuesExtractor(token_dim=token_dim,head_dim=head_dim,n_heads=n_heads)\n",
        "        self.concatenated_head_dim = n_heads*head_dim\n",
        "        \n",
        "        self.attention_to_output = nn.Linear(in_features=self.concatenated_head_dim,out_features=token_dim)\n",
        "        \n",
        "        self.output_dropout = nn.Dropout(p=droupout_p)\n",
        "        \n",
        "        \n",
        "    def forward(self, input: tensor) -> tensor:\n",
        "        \n",
        "        batch_size, n_tokens, token_dim = input.shape\n",
        "        querys, keys, values = self.query_key_value_extractor(input)\n",
        "        \n",
        "        attention = get_attention(queries=querys,keys=keys,values=values)\n",
        "        \n",
        "        attention = attention.transpose(1,2).reshape(batch_size,n_tokens,self.concatenated_head_dim)\n",
        "        \n",
        "        output = self.attention_to_output(attention)\n",
        "        output = self.output_dropout(output)\n",
        "        \n",
        "        return output\n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_IVspfhsmq3"
      },
      "source": [
        "### Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HowvAknzsmq4"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer Block\"\"\"\n",
        "    \n",
        "    def __init__(self, token_dim: int, multihead_attention_head_dim: int, multihead_attention_n_heads: int, \n",
        "                 multilayer_perceptron_hidden_dim: int, dropout_p: float) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layer_norm_1 = nn.LayerNorm(normalized_shape=token_dim)\n",
        "        self.multi_head_attention = Multiheadselfattention(token_dim=token_dim,head_dim=multihead_attention_head_dim,\n",
        "                                                           n_heads=multihead_attention_n_heads,droupout_p=dropout_p)\n",
        "        \n",
        "        self.layer_norm_2 = nn.LayerNorm(normalized_shape= token_dim)\n",
        "        \n",
        "        self.multilayer_perceptron = MultilayerPerceptron(in_dim=token_dim,hidden_dim=multilayer_perceptron_hidden_dim,\n",
        "                                                          out_dim=token_dim,droupout_p=dropout_p)\n",
        "        \n",
        "    def forward(self, input: tensor) -> tensor:\n",
        "        \"\"\"Runs the input through transformer block\"\"\"\n",
        "        \n",
        "        residual = input\n",
        "        output = self.layer_norm_1(input)\n",
        "        output = self.multi_head_attention(output)\n",
        "        output += residual\n",
        "        \n",
        "        residual = output\n",
        "        output = self.layer_norm_2(output)\n",
        "        output = self.multilayer_perceptron(output)\n",
        "        output += residual\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grcWG55wsmq4"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"Transformer Encoder\"\"\"\n",
        "    \n",
        "    def __init__(self, n_layers: int, token_dim: int, multihead_attention_head_dim: int,\n",
        "                 multihead_attention_n_heads: int, mulitlayer_perceptron_hidden_dim : int, dropout_p : float) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        transformer_blocks =[]\n",
        "        for i in range(1, n_layers+1):\n",
        "            transformer_block = TransformerBlock(token_dim=token_dim,multihead_attention_head_dim=multihead_attention_head_dim,\n",
        "                                                 multihead_attention_n_heads=multihead_attention_n_heads,multilayer_perceptron_hidden_dim=\n",
        "                                                 mulitlayer_perceptron_hidden_dim,dropout_p=dropout_p)\n",
        "            transformer_block = (f'transformer_block_{i}',transformer_block)\n",
        "            \n",
        "            transformer_blocks.append(transformer_block)\n",
        "            \n",
        "        transformer_blocks = OrderedDict(transformer_blocks)\n",
        "        self.transformer_blocks = nn.Sequential(transformer_blocks)\n",
        "        \n",
        "        \n",
        "    def forward(self, input: tensor) -> tensor:\n",
        "        \n",
        "        output = self.transformer_blocks(input)\n",
        "        \n",
        "        return output\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tO_VTibvsmq5"
      },
      "outputs": [],
      "source": [
        "class visionTransformer(nn.Module):\n",
        "    \"\"\"Vision Transformer\"\"\"\n",
        "    \n",
        "    def __init__(self, token_dim: int, patch_size: int, image_size: int, n_layers: int, multihead_attention_head_dim : int\n",
        "                 ,multihead_attention_n_heads: int, multilayer_perceptron_hidden_dim: int, dropout_p: float, n_classes: int) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        self.tokenizer = Tokenizer(token_dim=token_dim,patch_size=patch_size)\n",
        "        self.class_token_concatenator = ClasstokenConcatenator(token_dim=token_dim)\n",
        "        n_tokens = (image_size//patch_size)**2 + 1\n",
        "        \n",
        "        self.position_embedding_adder = PositionEmbeddingAdder(n_token=n_tokens,token_dim=token_dim)\n",
        "        \n",
        "        self.transformer = Transformer(n_layers=n_layers,token_dim=token_dim,multihead_attention_head_dim=multihead_attention_head_dim\n",
        "                                       ,multihead_attention_n_heads=multihead_attention_n_heads,mulitlayer_perceptron_hidden_dim=multilayer_perceptron_hidden_dim,\n",
        "                                       dropout_p=dropout_p)\n",
        "        \n",
        "        self.head = nn.Linear(in_features=token_dim,out_features=n_classes)\n",
        "        \n",
        "        \n",
        "    def forward(self, input: tensor) -> tensor:\n",
        "        \n",
        "        output = self.tokenizer(input)\n",
        "        output = self.class_token_concatenator(output)\n",
        "        output = self.position_embedding_adder(output)\n",
        "        output = self.transformer(output)\n",
        "        output = output[:,0]\n",
        "        \n",
        "        output = self.head(output)\n",
        "        return output\n",
        "        \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = visionTransformer(token_dim=48,patch_size=4,image_size=32,n_layers=4,multihead_attention_head_dim=48,multihead_attention_n_heads=8\n",
        "                          ,multilayer_perceptron_hidden_dim=512,dropout_p=0.1,n_classes=10)\n"
      ],
      "metadata": {
        "id": "5V_KaE41gS0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn4R1Uqksmq6",
        "outputId": "8c132131-679a-4de9-db05-4bf65e815c46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,   200] loss: 1.768\n",
            "Accuracy of the network on the train images: 40 %\n",
            "Accuracy of the network on the 10000 test images: 47 %\n",
            "[2,   200] loss: 1.326\n",
            "Accuracy of the network on the train images: 53 %\n",
            "Accuracy of the network on the 10000 test images: 56 %\n",
            "[3,   200] loss: 1.117\n",
            "Accuracy of the network on the train images: 60 %\n",
            "Accuracy of the network on the 10000 test images: 59 %\n",
            "[4,   200] loss: 0.974\n",
            "Accuracy of the network on the train images: 64 %\n",
            "Accuracy of the network on the 10000 test images: 60 %\n",
            "[5,   200] loss: 0.857\n",
            "Accuracy of the network on the train images: 68 %\n",
            "Accuracy of the network on the 10000 test images: 61 %\n",
            "[6,   200] loss: 0.753\n",
            "Accuracy of the network on the train images: 72 %\n",
            "Accuracy of the network on the 10000 test images: 62 %\n",
            "[7,   200] loss: 0.654\n",
            "Accuracy of the network on the train images: 75 %\n",
            "Accuracy of the network on the 10000 test images: 61 %\n",
            "[8,   200] loss: 0.541\n",
            "Accuracy of the network on the train images: 79 %\n",
            "Accuracy of the network on the 10000 test images: 62 %\n",
            "[9,   200] loss: 0.459\n",
            "Accuracy of the network on the train images: 82 %\n",
            "Accuracy of the network on the 10000 test images: 63 %\n",
            "[10,   200] loss: 0.364\n",
            "Accuracy of the network on the train images: 85 %\n",
            "Accuracy of the network on the 10000 test images: 61 %\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.Resize((32, 32)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "#Training of the Vision Transformer \n",
        "\n",
        "# model = visionTransformer(token_dim=192,patch_size=8,image_size=32,n_layers=4,multihead_attention_head_dim=192,multihead_attention_n_heads=8\n",
        "#                           ,multilayer_perceptron_hidden_dim=512,dropout_p=0.2,n_classes=10)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer =optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        loss = criterion(outputs,labels)\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        _,predicted = torch.max(outputs.data,1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "    print('Accuracy of the network on the train images: %d %%' % (100 * correct / total))\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # model.eval()\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
        "\n",
        "            \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HEzIgnDhs4Rm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vir_envs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}